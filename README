# ExtComAbs

* 2020-9-15
  * commitid:之前仓库追踪的有问题，覆盖了
  * 执行脚本:./PrepareDataset --dataset --datadir (--dataset:dataset name,--datadi:dataset path)  
  * 结果:创建词表

* 2020-9-17
  * commitid:b5906d9b2ff682bf0358cb3c591f66d6a3340019
  * 执行脚本:python train.py --cuda --gpu 0 --data_dir <data dir of your json-format dataset> --cache_dir <cache directory of vocab>  --save_root <model path> --log_root <log path> --lr_descent --grad_clip -m 3
  * 结果:目前文档和句子长度较小时可以train起来，但是长度长了就会OOM